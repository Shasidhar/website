<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
	<channel>
		<title>Shashidhar</title>
		<description>Blog and website of Shashidhar, blogging mainly for tech. Opinions expressed are mine.</description>
		<link>https://shasidhare.com</link>
		<atom:link href="https://shasidhare.com/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Building Spark Streaming App - Part 5 (Kafka Integration)</title>
				
				
					<description>&lt;p&gt;In previous &lt;a href=&quot;http://shashidhare.com/spark/2016/05/08/building-spark-streaming-app-part-4.html&quot;&gt;blog&lt;/a&gt; we discussed about how we integrate zeppelin to our streaming processing engine for visualizing the results. You may be thinking we already have the complete working streaming application, why we need to develop more for the same application. From this blog its more of productionizing the application, where we solve some of the critical pieces in the system.&lt;/p&gt;

</description>
				
				<pubDate>Fri, 08 Jul 2016 03:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2016/07/08/building-spark-streaming-app-part-5.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2016/07/08/building-spark-streaming-app-part-5.html</guid>
			</item>
		
			<item>
				<title>Building Spark Streaming App - Part 4 (Zeppelin Visualization)</title>
				
				
					<description>&lt;p&gt;In previous &lt;a href=&quot;http://shashidhare.com/spark/2016/05/08/building-spark-streaming-app-part-2.html&quot;&gt;blog&lt;/a&gt; we discussed about how we populate the results in output file. As I mentioned in last blog , we will be discussing about how we show the results to the business user. In my first blog of the series we decided to use Zepplin as an interface to show end results. So, let’s see how we show results in Zepplin.&lt;/p&gt;

</description>
				
				<pubDate>Thu, 19 May 2016 03:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2016/05/19/building-spark-streaming-app-part-4.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2016/05/19/building-spark-streaming-app-part-4.html</guid>
			</item>
		
			<item>
				<title>Building Spark Streaming App - Part 3 (First Iteration)</title>
				
				
					<description>&lt;p&gt;In previous &lt;a href=&quot;http://shashidhare.com/spark/2016/05/08/building-spark-streaming-app-part-2.html&quot;&gt;blog&lt;/a&gt; we have discussed  about data formats we are dealing with and the results we have to generate. In this blog we will focus on implementing the same. Let’s see the first method of implementation, i.e.  &lt;strong&gt;&lt;em&gt;Iteration 1&lt;/em&gt;&lt;/strong&gt;. Going forward, we will discuss on required changes on this implementation and need for the changes. 
&lt;strong&gt;Streaming Components&lt;/strong&gt;&lt;/p&gt;

</description>
				
				<pubDate>Thu, 19 May 2016 03:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2016/05/19/building-spark-streaming-app-part-3.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2016/05/19/building-spark-streaming-app-part-3.html</guid>
			</item>
		
			<item>
				<title>Building Spark Streaming App - Part 2 (Data Format,Types of Aggregations)</title>
				
				
					<description>&lt;p&gt;In previous &lt;a href=&quot;http://shashidhare.com/spark/2016/04/08/building-spark-streaming-app-part-1.html&quot;&gt;blog&lt;/a&gt; we have discussed  about application, a brief overview of the business problem and high level requirements. In this blog we will focus on specifics like what is the required data format? What are the types of aggregations we are interested in?&lt;/p&gt;

</description>
				
				<pubDate>Sun, 08 May 2016 03:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2016/05/08/building-spark-streaming-app-part-2.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2016/05/08/building-spark-streaming-app-part-2.html</guid>
			</item>
		
			<item>
				<title>Building Spark Streaming App - Part 1 (Introduction to Business Case)</title>
				
				
					<description>&lt;p&gt;&lt;a href=&quot;http://spark.apache.org/streaming/&quot;&gt;Spark Streaming&lt;/a&gt; is the most widely used framework to develop near real time applications in present times. It comes with lot of advantages compared to its competeting streaming frameworks available in open source world.&lt;/p&gt;

</description>
				
				<pubDate>Fri, 08 Apr 2016 03:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2016/04/08/building-spark-streaming-app-part-1.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2016/04/08/building-spark-streaming-app-part-1.html</guid>
			</item>
		
			<item>
				<title>Building End to End Spark Streaming Application</title>
				
				
					<description>&lt;p&gt;The below video is a recording of my talk on &lt;em&gt;Building End to End Spark Streaming Application&lt;/em&gt; in recent spark meet up. This talk is focused upon Streaming application development in Spark Streaming. Also you can see how application is evolved over a period of time along with integrations of different components with Spark Streaming. There is a special section where you can also see unit testing of Spark Streaming application.&lt;/p&gt;

</description>
				
				<pubDate>Mon, 28 Mar 2016 03:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2016/03/28/building-end-to-end-spark-streaming-application.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2016/03/28/building-end-to-end-spark-streaming-application.html</guid>
			</item>
		
			<item>
				<title>Zookeeper Offset Management In Kafka Direct Stream</title>
				
				
					<description>&lt;p&gt;Kafka is the most popular source for Spark streaming application because of various benefits we get from Kafka. As you know Spark streaming has the concept of &lt;strong&gt;Receivers&lt;/strong&gt; which is responsible to receive data from the input sources of any streaming application and store it in memory for processing. But there is a special approach supported by Spark Streaming engine for Kafka input source which is called as &lt;strong&gt;Direct Approach (No Receivers)&lt;/strong&gt;&lt;/p&gt;

</description>
				
				<pubDate>Sat, 05 Mar 2016 03:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2016/03/05/zookeeper-offset-management-in-kafka-direct-stream.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2016/03/05/zookeeper-offset-management-in-kafka-direct-stream.html</guid>
			</item>
		
			<item>
				<title>Unit testing Spark Streaming Receiver</title>
				
				
					<description>&lt;p&gt;Spark is the most popular platform for all  Bigdata needs now a days. As per the recent trend, Spark is  defacto for Bigdata needs and quite popular in production as well. What I really mean is, people are pretty comfortable with the development of Spark solutions as Spark exposes pretty simple and powerful API&amp;#8217;s for the programmers to work with. But there are many more things in a Software process , like testing, code quality checks etc. But being developers we need to show how our code can be tested through some basic Unit tests. This blog tends to focus on Unit testing Spark.&lt;/p&gt;

</description>
				
				<pubDate>Wed, 20 Jan 2016 02:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2016/01/20/unit-test-spark-streaming-receiver.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2016/01/20/unit-test-spark-streaming-receiver.html</guid>
			</item>
		
			<item>
				<title>Apache Spark Performance Tuning</title>
				
				
					<description>&lt;p&gt;As we all know Apache Spark is the  most popular framework now a days for Big Data solutions, in this blog we will try to analyse different key factors that can be used to improve the performance of Spark applications in production.&lt;/p&gt;

</description>
				
				<pubDate>Tue, 03 Nov 2015 02:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2015/11/03/apache-spark-performance-tuning.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2015/11/03/apache-spark-performance-tuning.html</guid>
			</item>
		
			<item>
				<title>Machine Learning in Spark</title>
				
				
					<description>&lt;p&gt;In software industry with adoption of Hadoop, data scientists are in high demand. There is a well known fact that people from data science background always face difficulty to apply data science on bigdata due to lack of bigdata knowledge and people from programming background face the same when they try data science on bigdata due to lack of data science knowledge. Here we are seeing two different set of people whose end goal &lt;strong&gt;Machine Learning on Big Data&lt;/strong&gt; appears similar. So we try to solve this and give you the correct steps to get started in this regard.&lt;/p&gt;

</description>
				
				<pubDate>Sat, 31 Oct 2015 02:02:27 +0530</pubDate>
				<link>https://shasidhare.com/spark/2015/10/31/machine-learning-in-spark.html</link>
				<guid isPermaLink="true">https://shasidhare.com/spark/2015/10/31/machine-learning-in-spark.html</guid>
			</item>
		
	</channel>
</rss>
